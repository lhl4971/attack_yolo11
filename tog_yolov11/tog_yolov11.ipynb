{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from generate_perturbation import generate_perturbation\n",
    "from transforms import resize_and_pad, image_to_tensor\n",
    "from tog_modules import TOG_DetectionValidator, TOG_DetectionPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model_1 = YOLO(\"yolo11n.pt\")\n",
    "model_2 = YOLO(\"yolo11n.pt\")\n",
    "data = [\n",
    "    \"./datasets/coco8/images/val/000000000036.jpg\",\n",
    "    \"./datasets/coco8/images/val/000000000042.jpg\",\n",
    "    \"./datasets/coco8/images/val/000000000049.jpg\",\n",
    "    \"./datasets/coco8/images/val/000000000061.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.28 üöÄ Python-3.12.7 torch-2.4.1 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients\n",
      "\n",
      "Dataset 'coco8.yaml' images not found ‚ö†Ô∏è, missing path '/home/liuhailin/codes/math_ML/datasets/coco8/images/val'\n",
      "Downloading https://ultralytics.com/assets/coco8.zip to '/home/liuhailin/codes/math_ML/datasets/coco8.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433k/433k [00:00<00:00, 3.04MB/s]\n",
      "Unzipping /home/liuhailin/codes/math_ML/datasets/coco8.zip to /home/liuhailin/codes/math_ML/datasets/coco8...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 6658.47file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success ‚úÖ (1.5s), saved to \u001b[1m/home/liuhailin/codes/math_ML/datasets\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liuhailin/codes/math_ML/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 484.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/liuhailin/codes/math_ML/datasets/coco8/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.167     0.0167     0.0917     0.0275\n",
      "                person          3         10          1        0.1       0.55      0.165\n",
      "                   dog          1          1          0          0          0          0\n",
      "                 horse          1          2          0          0          0          0\n",
      "              elephant          1          2          0          0          0          0\n",
      "              umbrella          1          1          0          0          0          0\n",
      "          potted plant          1          1          0          0          0          0\n",
      "Speed: 768.1ms preprocess, 16.4ms inference, 0.0ms loss, 30.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n",
      "mAP50-95: 0.0275\n",
      "mAP50: 0.0917\n",
      "mAP75: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n",
    "metrics = model.val(data='coco8.yaml', imgsz=640, device=0, validator=TOG_DetectionValidator)\n",
    "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP75: {metrics.box.map75:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(1, 26):\n",
    "    image_input = Image.open(\"datasets/test/\" + str(i) + \".png\")\n",
    "    origin = image_to_tensor(resize_and_pad(image_input, 640))\n",
    "    attacked, noise = generate_perturbation(origin)\n",
    "    img_att = Image.fromarray(np.uint8(attacked.cpu().numpy().transpose((0, 2, 3, 1))[0]*256))\n",
    "    img_att.save(\"datasets/adv_test/\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 1 umbrella, 13.8ms\n",
      "Speed: 0.8ms preprocess, 13.8ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 9.9ms\n",
      "Speed: 714.0ms preprocess, 9.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 dog, 1 suitcase, 11.5ms\n",
      "Speed: 1.1ms preprocess, 11.5ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 9.8ms\n",
      "Speed: 744.9ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 persons, 2 horses, 9.8ms\n",
      "Speed: 0.7ms preprocess, 9.8ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 10.2ms\n",
      "Speed: 748.8ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 dogs, 10.4ms\n",
      "Speed: 0.8ms preprocess, 10.4ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 9.4ms\n",
      "Speed: 701.0ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fig, axs = plt.subplots(figsize=(15, 15), nrows=5, ncols=len(data))\n",
    "for i in range(len(data)):\n",
    "    image_input = Image.open(data[i])\n",
    "    origin = image_to_tensor(resize_and_pad(image_input, 640))\n",
    "    attacked, noise = generate_perturbation(origin)\n",
    "    img_att = Image.fromarray(np.uint8(attacked.cpu().numpy().transpose((0, 2, 3, 1))[0]*256))\n",
    "    img_att.save(\"test.jpg\")\n",
    "    \n",
    "    result_origin = model_1.predict(origin, imgsz=(640, 640))\n",
    "    result_attacked = model_2.predict(origin, imgsz=(640, 640), predictor=TOG_DetectionPredictor)\n",
    "    axs[0, i].imshow(origin.numpy().transpose((0, 2, 3, 1))[0])\n",
    "    axs[0, i].set_title(\"Origin Image\")\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(attacked.cpu().numpy().transpose((0, 2, 3, 1))[0])\n",
    "    axs[1, i].set_title(\"Attacked Detection\")\n",
    "    axs[1, i].axis('off')\n",
    "    noise_zoomed = noise.cpu().numpy().transpose((0, 2, 3, 1))[0]\n",
    "    axs[2, i].imshow((noise_zoomed - noise_zoomed.min()) / (noise_zoomed.max() - noise_zoomed.min()))\n",
    "    axs[2, i].set_title(\"Attack Noise\")\n",
    "    axs[2, i].axis('off')\n",
    "    axs[3, i].imshow(result_origin[0].plot(line_width=2))\n",
    "    axs[3, i].set_title(\"Normal Result\")\n",
    "    axs[3, i].axis('off')\n",
    "    axs[4, i].imshow(result_attacked[0].plot(line_width=2))\n",
    "    axs[4, i].set_title(\"Attacked Result\")\n",
    "    axs[4, i].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"./output/output.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.28 üöÄ Python-3.12.7 torch-2.4.1 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liuhailin/codes/math_ML/datasets/coco/labels/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:05<00:00, 834.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/liuhailin/codes/math_ML/datasets/coco/labels/val2017.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [1:08:43<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5000      36335      0.029     0.0522     0.0244     0.0138\n",
      "                person       2693      10777     0.0852      0.165     0.0807     0.0378\n",
      "               bicycle        149        314     0.0183     0.0573     0.0158    0.00812\n",
      "                   car        535       1918     0.0548     0.0386     0.0354     0.0193\n",
      "            motorcycle        159        367     0.0557     0.0708      0.063     0.0279\n",
      "              airplane         97        143     0.0453     0.0839     0.0484     0.0239\n",
      "                   bus        189        283      0.108     0.0601     0.0873     0.0486\n",
      "                 train        157        190     0.0329      0.132     0.0474     0.0206\n",
      "                 truck        250        414     0.0394      0.099     0.0316     0.0146\n",
      "                  boat        121        424    0.00924     0.0118    0.00632    0.00292\n",
      "         traffic light        191        634     0.0169     0.0158     0.0138       0.01\n",
      "          fire hydrant         86        101    0.00909      0.119     0.0122    0.00678\n",
      "             stop sign         69         75     0.0649      0.133     0.0631     0.0532\n",
      "         parking meter         37         60     0.0123     0.0333     0.0233     0.0122\n",
      "                 bench        235        411     0.0123     0.0389      0.012    0.00474\n",
      "                  bird        125        427    0.00615     0.0422     0.0045    0.00264\n",
      "                   cat        184        202     0.0268      0.109     0.0302     0.0112\n",
      "                   dog        177        218     0.0202      0.101     0.0131    0.00595\n",
      "                 horse        128        272     0.0398      0.143     0.0414     0.0198\n",
      "                 sheep         65        354     0.0162     0.0367     0.0134    0.00536\n",
      "                   cow         87        372     0.0223     0.0403     0.0124    0.00709\n",
      "              elephant         89        252     0.0239     0.0794     0.0154    0.00844\n",
      "                  bear         49         71      0.049     0.0986     0.0571     0.0414\n",
      "                 zebra         85        266     0.0696      0.248     0.0963     0.0439\n",
      "               giraffe        101        232     0.0497      0.164     0.0769      0.036\n",
      "              backpack        228        371    0.00484     0.0108    0.00258    0.00121\n",
      "              umbrella        174        407     0.0201     0.0295     0.0129    0.00669\n",
      "               handbag        292        540    0.00369    0.00926    0.00218    0.00111\n",
      "                   tie        145        252     0.0238     0.0675     0.0181    0.00812\n",
      "              suitcase        105        299     0.0154     0.0201    0.00824    0.00348\n",
      "               frisbee         84        115     0.0556     0.0957      0.052     0.0345\n",
      "                  skis        120        241     0.0231     0.0705     0.0191    0.00827\n",
      "             snowboard         49         69    0.00199     0.0145    0.00136   0.000678\n",
      "           sports ball        169        260     0.0476      0.108     0.0568     0.0362\n",
      "                  kite         91        327     0.0161      0.052     0.0096    0.00578\n",
      "          baseball bat         97        145     0.0221     0.0828     0.0329     0.0111\n",
      "        baseball glove        100        148     0.0423     0.0743     0.0327     0.0189\n",
      "            skateboard        127        179     0.0158     0.0615     0.0128    0.00731\n",
      "             surfboard        149        267     0.0541     0.0599     0.0401     0.0195\n",
      "         tennis racket        167        225     0.0263      0.129       0.03     0.0147\n",
      "                bottle        379       1013    0.00883    0.00987    0.00449    0.00186\n",
      "            wine glass        110        341     0.0121     0.0176    0.00645    0.00334\n",
      "                   cup        390        895      0.013     0.0112     0.0066     0.0052\n",
      "                  fork        155        215     0.0276     0.0233     0.0232     0.0183\n",
      "                 knife        181        325     0.0126    0.00615    0.00876    0.00537\n",
      "                 spoon        153        253          0          0          0          0\n",
      "                  bowl        314        623     0.0331     0.0257     0.0175     0.0111\n",
      "                banana        103        370     0.0161     0.0135     0.0131    0.00457\n",
      "                 apple         76        236    0.00813    0.00424     0.0045    0.00315\n",
      "              sandwich         98        177     0.0316     0.0169     0.0163    0.00765\n",
      "                orange         85        285     0.0203      0.014     0.0109     0.0072\n",
      "              broccoli         71        312     0.0517     0.0192     0.0313     0.0151\n",
      "                carrot         81        365     0.0357     0.0219     0.0198     0.0128\n",
      "               hot dog         51        125     0.0337      0.024     0.0175     0.0116\n",
      "                 pizza        153        284      0.151     0.0951       0.12     0.0838\n",
      "                 donut         62        328     0.0401     0.0457     0.0228     0.0133\n",
      "                  cake        124        310    0.00536     0.0161    0.00315    0.00167\n",
      "                 chair        580       1771     0.0112     0.0169    0.00585    0.00279\n",
      "                 couch        195        261     0.0171     0.0728     0.0179     0.0106\n",
      "          potted plant        172        342    0.00399     0.0205    0.00703    0.00275\n",
      "                   bed        149        163      0.013      0.104    0.00996     0.0067\n",
      "          dining table        501        695     0.0471     0.0676      0.033     0.0241\n",
      "                toilet        149        179     0.0274     0.0335     0.0183      0.013\n",
      "                    tv        207        288     0.0429     0.0208     0.0231      0.011\n",
      "                laptop        183        231     0.0787     0.0303     0.0589     0.0332\n",
      "                 mouse         88        106     0.0455    0.00943      0.023      0.023\n",
      "                remote        145        283    0.00647    0.00707    0.00385    0.00205\n",
      "              keyboard        106        153      0.101     0.0588     0.0711     0.0513\n",
      "            cell phone        214        262     0.0346     0.0305     0.0233     0.0126\n",
      "             microwave         54         55          0          0          0          0\n",
      "                  oven        115        143          0          0          0          0\n",
      "               toaster          8          9          0          0          0          0\n",
      "                  sink        187        225     0.0209     0.0178     0.0112    0.00548\n",
      "          refrigerator        101        126     0.0435     0.0159     0.0273      0.019\n",
      "                  book        230       1129     0.0114    0.00797    0.00575    0.00325\n",
      "                 clock        204        267    0.00472     0.0524     0.0129    0.00944\n",
      "                  vase        137        274    0.00203     0.0109     0.0013   0.000686\n",
      "              scissors         28         36    0.00251     0.0278    0.00143   0.000857\n",
      "            teddy bear         94        190    0.00665     0.0789    0.00775    0.00321\n",
      "            hair drier          9         11     0.0294     0.0909     0.0212     0.0127\n",
      "            toothbrush         34         57       0.01     0.0351    0.00562    0.00221\n",
      "Speed: 815.4ms preprocess, 5.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Saving runs/detect/val21/predictions.json...\n",
      "\n",
      "Evaluating pycocotools mAP using runs/detect/val21/predictions.json and /home/liuhailin/codes/math_ML/datasets/coco/annotations/instances_val2017.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=5.15s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.57s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.052\n",
      "Results saved to \u001b[1mruns/detect/val21\u001b[0m\n",
      "mAP50-95: 0.0138\n",
      "mAP50: 0.0244\n",
      "mAP75: 0.0136\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n",
    "metrics = model.val(data='coco.yaml', imgsz=640, device=0, validator=TOG_DetectionValidator)\n",
    "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP75: {metrics.box.map75:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
